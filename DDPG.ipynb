{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88fd962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.core import ObservationWrapper\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Kickenv1(gym.Env):\n",
    "    \n",
    "\n",
    "    def __init__(self,di :float,vp : float,lenbar :float, wbar:float,model):\n",
    "        '''Constructor for our environment. Should take any relevant parameters as arguments.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        di : float\n",
    "            diameter of puck\n",
    "        vp : float\n",
    "            horizontal velocity of puck\n",
    "        lenbar : float \n",
    "            length of the bar\n",
    "        wbar : float \n",
    "            width of the bar\n",
    "\n",
    "        Ideal Settings :\n",
    "        BallDiameter = ScreenWidth / 64;\n",
    "        BarWidth = Settings.BallDiameter / 2;\n",
    "        BarLength = ScreenHeight / 6;\n",
    "        BallStartingPosX = ScreenWidth / 8;\n",
    "        BallStartingPosY = ScreenHeight / 2;\n",
    "        BarStartingPosX = 7*ScreenWidth/8;\n",
    "        BarStartingPosY = BallStartingPosY;\n",
    "        FinalLine =BarStartingPosX + 3*BarWidth;        \n",
    "        BallSpeed = ScreenWidth / 100\n",
    "        '''\n",
    "        # For simplicity only allow odd number of states.\n",
    "        self.xi=-0.75\n",
    "        self.yi=0\n",
    "        \n",
    "        self.xib=0.75\n",
    "        self.yib=0\n",
    "        self.r=di/2\n",
    "        self.g=0.77\n",
    "        self.vbar=0\n",
    "        self.theta=1\n",
    "        self.vp=vp\n",
    "        self.lenbar=lenbar\n",
    "        self.wbar=wbar    \n",
    "        self.statep=[self.xi,self.yi]\n",
    "        self.stateb=[self.xib,self.yib]\n",
    "        self.timestepcounter=0\n",
    "        self.theta_arr=np.zeros(math.ceil((self.g-self.xi)/self.vp))\n",
    "        self.action_space = spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=np.array([-1.0+self.r,-1.0+self.r,-1+self.lenbar/2,]),high=np.array([1.0-self.r,1.0-self.r,1-(self.lenbar/2),]),shape=(3,))\n",
    "        self.action_spacep=spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.action_spaceb=spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.observation_spacep=spaces.Box(low=np.array([-1.0+self.r,-1.0+self.r]),high=np.array([1.0-self.r,1.0-self.r]),shape=(2,))\n",
    "        self.observation_spaceb=spaces.Box(low=-1+self.lenbar/2,high=1-(self.lenbar/2),shape=(1,))\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        self.rewardp=0\n",
    "        self.statepr=self.statep\n",
    "        self.rewardb=0\n",
    "        self.model = model\n",
    "    def step(self,a) :\n",
    "        '''Defines what to do if an action is taken.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : list of actions taken by the puck and bar respectively\n",
    "            Action  taken.  \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[int, float, bool, None]\n",
    "            A tuple containing the next state, reward obtained, whether terminal state has been reached, and None.\n",
    "        '''\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.donep :return np.array(self.statep+ [self.stateb[1]],dtype = float),self.rewardp,self.donep,{}\n",
    "        \n",
    "        action = np.zeros(2)\n",
    "        action[0] = a\n",
    "        if isinstance(self.model,int) :action[1] = 0.1\n",
    "        else: action[1] = self.model.predict(np.array(self.statep+ [self.stateb[1]],dtype = float))[0]\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        posbar=self.stateb\n",
    "        self.theta_arr[self.timestepcounter]=action[1]\n",
    "        action[0]=action[0]*self.vp\n",
    "        if(self.timestepcounter>=2):\n",
    "            if action[1]>=0.8:\n",
    "                if self.theta_arr[self.timestepcounter-1]>=0.8 and self.theta_arr[self.timestepcounter-2]>=0.8 :\n",
    "                    self.theta=self.theta+0.85\n",
    "            elif action[1]<=-0.8 :\n",
    "                if self.theta_arr[self.timestepcounter-1]<=-0.8 and self.theta_arr[self.timestepcounter-2]<=0.8 :\n",
    "                    self.theta=self.theta+0.85\n",
    "            else : self.theta=1\n",
    "        else : self.theta=1            \n",
    "        vw=self.theta*self.vp*(2/3)\n",
    "        statebr=self.stateb\n",
    "        self.stateb[1]=max(self.observation_spaceb.low,min(self.observation_spaceb.high,self.stateb[1]+action[1]*vw))\n",
    "        self.statepr=self.statep\n",
    "        self.statep[0]=self.statep[0]+self.vp\n",
    "        self.statep[1]=max(self.observation_spacep.low[1],min(self.observation_spacep.high[1],self.statep[1]+action[0]))\n",
    "        self.rewardp=0\n",
    "        self.rewardb=0\n",
    "        if abs(self.statep[0]-posbar[0])<=self.r+self.wbar/2 and abs(self.statep[1]-posbar[1])<=self.r+self.lenbar/2 :\n",
    "            self.rewardp=-1\n",
    "            self.rewardb=1\n",
    "            self.donep=True\n",
    "        if self.statep[0]>=self.xib and self.statepr[0]<=self.xib and self.donep==False:\n",
    "            self.donep=True\n",
    "            self.rewardp=1\n",
    "            self.rewardb=-1\n",
    "            self.statep[0]=self.xib-self.wbar-self.r \n",
    "        if self.statep[0]+self.r>=self.g and self.donep==False:\n",
    "            self.donep=True\n",
    "            self.rewardp=1\n",
    "            self.rewardb=-1      \n",
    "        \n",
    "#         print(self.statep)\n",
    "#         print(self.stateb[1])\n",
    "        #print(np.array(self.statep+ [self.stateb[1]],dtype = float))\n",
    "        #print(self.rewardp)\n",
    "        return np.array(self.statep+ [self.stateb[1]],dtype = float),self.rewardp,self.donep,{}\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''What to do if we reset the environment.\n",
    "        '''\n",
    "        # We simply make our current state equal to initial state .\n",
    "        # We change the status of completion of our episode[self.done variable] to False.\n",
    "        # make timestepcounter=0\n",
    "        #change parameters such a theta and prev_dir to 1 and 0 respectively.\n",
    "        # Return state and status of completion \n",
    "        self.statep =[self.xi,self.yi]\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        self.stateb=[self.xib,self.yib]\n",
    "        self.prev_dir=0\n",
    "        self.theta=1\n",
    "        self.theta_arr=np.zeros(math.ceil((self.g-self.xi)/self.vp))\n",
    "        self.statepr=self.statep\n",
    "        self.timestepcounter=0\n",
    "        return np.array(self.statep+ [self.stateb[1]],dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3830d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kickenv2(gym.Env):\n",
    "    \n",
    "\n",
    "    def __init__(self,di :float,vp : float,lenbar :float, wbar:float,model):\n",
    "        '''Constructor for our environment. Should take any relevant parameters as arguments.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        di : float\n",
    "            diameter of puck\n",
    "        vp : float\n",
    "            horizontal velocity of puck\n",
    "        lenbar : float \n",
    "            length of the bar\n",
    "        wbar : float \n",
    "            width of the bar\n",
    "\n",
    "        Ideal Settings :\n",
    "        BallDiameter = ScreenWidth / 64;\n",
    "        BarWidth = Settings.BallDiameter / 2;\n",
    "        BarLength = ScreenHeight / 6;\n",
    "        BallStartingPosX = ScreenWidth / 8;\n",
    "        BallStartingPosY = ScreenHeight / 2;\n",
    "        BarStartingPosX = 7*ScreenWidth/8;\n",
    "        BarStartingPosY = BallStartingPosY;\n",
    "        FinalLine =BarStartingPosX + 3*BarWidth;        \n",
    "        BallSpeed = ScreenWidth / 100\n",
    "        '''\n",
    "        # For simplicity only allow odd number of states.\n",
    "        self.xi=-0.75\n",
    "        self.yi=0\n",
    "        \n",
    "        self.xib=0.75\n",
    "        self.yib=0\n",
    "        self.r=di/2\n",
    "        self.g=0.77\n",
    "        self.vbar=0\n",
    "        self.theta=1\n",
    "        self.vp=vp\n",
    "        self.lenbar=lenbar\n",
    "        self.wbar=wbar    \n",
    "        self.statep=[self.xi,self.yi]\n",
    "        self.stateb=[self.xib,self.yib]\n",
    "        self.timestepcounter=0\n",
    "        self.theta_arr=np.zeros(math.ceil((self.g-self.xi)/self.vp))\n",
    "        self.action_space = spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=np.array([-1.0+self.r,-1.0+self.r,-1+self.lenbar/2,]),high=np.array([1.0-self.r,1.0-self.r,1-(self.lenbar/2),]),shape=(3,))\n",
    "        self.action_spacep=spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.action_spaceb=spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.observation_spacep=spaces.Box(low=np.array([-1.0+self.r,-1.0+self.r]),high=np.array([1.0-self.r,1.0-self.r]),shape=(2,))\n",
    "        self.observation_spaceb=spaces.Box(low=-1+self.lenbar/2,high=1-(self.lenbar/2),shape=(1,))\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        self.rewardp=0\n",
    "        self.statepr=self.statep\n",
    "        self.rewardb=0\n",
    "        self.model = model\n",
    "    def step(self,a) :\n",
    "        '''Defines what to do if an action is taken.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : list of actions taken by the puck and bar respectively\n",
    "            Action  taken.  \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[int, float, bool, None]\n",
    "            A tuple containing the next state, reward obtained, whether terminal state has been reached, and None.\n",
    "        '''\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        action = np.zeros(2)\n",
    "        action[1] = a\n",
    "        #print(np.array(self.statep+ [self.stateb[1]]))\n",
    "        if isinstance(self.model,int) :action[0] = 0.1\n",
    "        else: action[0] = self.model.predict(np.array(self.statep+ [self.stateb[1]],dtype = float))[0]\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        posbar=self.stateb\n",
    "        self.theta_arr[self.timestepcounter]=action[1]\n",
    "        action[0]=action[0]*self.vp\n",
    "        if(self.timestepcounter>=2):\n",
    "            if action[1]>=0.8:\n",
    "                if self.theta_arr[self.timestepcounter-1]>=0.8 and self.theta_arr[self.timestepcounter-2]>=0.8 :\n",
    "                    self.theta=self.theta+0.85\n",
    "            elif action[1]<=-0.8 :\n",
    "                if self.theta_arr[self.timestepcounter-1]<=-0.8 and self.theta_arr[self.timestepcounter-2]<=0.8 :\n",
    "                    self.theta=self.theta+0.85\n",
    "            else : self.theta=1\n",
    "        else : self.theta=1            \n",
    "        vw=self.theta*self.vp*(2/3)\n",
    "        statebr=self.stateb\n",
    "        self.stateb[1]=max(self.observation_spaceb.low,min(self.observation_spaceb.high,self.stateb[1]+action[1]*vw))\n",
    "        self.statepr=self.statep\n",
    "        self.statep[0]=self.statep[0]+self.vp\n",
    "        self.statep[1]=max(self.observation_spacep.low[1],min(self.observation_spacep.high[1],self.statep[1]+action[0]))\n",
    "        self.rewardp=0\n",
    "        self.rewardb=0\n",
    "        if abs(self.statep[0]-posbar[0])<=self.r+self.wbar/2 and abs(self.statep[1]-posbar[1])<=self.r+self.lenbar/2 :\n",
    "            self.rewardp=-1\n",
    "            self.rewardb=1\n",
    "            self.donep=True\n",
    "        if self.statep[0]>=self.xib and self.statepr[0]<=self.xib and self.donep==False:\n",
    "            self.donep=True\n",
    "            self.rewardp=1\n",
    "            self.rewardb=-1\n",
    "            self.statep[0]=self.xib-self.wbar-self.r \n",
    "        if self.statep[0]+self.r>=self.g and self.donep==False:\n",
    "            self.donep=True\n",
    "            self.rewardp=1\n",
    "            self.rewardb=-1      \n",
    "        \n",
    "        #print(np.array(self.statep+ [self.stateb[1]],dtype = float))\n",
    "        return np.array(self.statep+ [self.stateb[1]],dtype = float),self.rewardb,self.donep,{}\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''What to do if we reset the environment.\n",
    "        '''\n",
    "        # We simply make our current state equal to initial state .\n",
    "        # We change the status of completion of our episode[self.done variable] to False.\n",
    "        # make timestepcounter=0\n",
    "        #change parameters such a theta and prev_dir to 1 and 0 respectively.\n",
    "        # Return state and status of completion \n",
    "        self.statep =[self.xi,self.yi]\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        self.stateb=[self.xib,self.yib]\n",
    "        self.prev_dir=0\n",
    "        self.theta=1\n",
    "        self.theta_arr=np.zeros(math.ceil((self.g-self.xi)/self.vp))\n",
    "        self.statepr=self.statep\n",
    "        self.timestepcounter=0\n",
    "        return np.array(self.statep+ [self.stateb[1]],dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0acf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a32c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f81c7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = Kickenv1(0.03,0.01,0.7,0.01,1)\n",
    "m11  = DDPG(\"MlpPolicy\", env1, verbose=0)\n",
    "env2 = Kickenv2(0.03,0.01,0.7,0.01,1)\n",
    "m12 = DDPG(\"MlpPolicy\", env2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6857069",
   "metadata": {},
   "outputs": [],
   "source": [
    "models1 = []\n",
    "models2 = []\n",
    "rewards1 = []\n",
    "rewards2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7f9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "m11.save(\"model1\")\n",
    "m12.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ead3cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward1:1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9c44ec303a0b>:115: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  return np.array(self.statep+ [self.stateb[1]],dtype = float),self.rewardb,self.donep,{}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward2:1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-be20918a5ff4>:127: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  return np.array(self.statep+ [self.stateb[1]],dtype = float),self.rewardp,self.donep,{}\n",
      "<ipython-input-2-be20918a5ff4>:86: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  else: action[1] = self.model.predict(np.array(self.statep+ [self.stateb[1]],dtype = float))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward1:1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9c44ec303a0b>:77: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  else: action[0] = self.model.predict(np.array(self.statep+ [self.stateb[1]],dtype = float))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward2:1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:1.00 +/- 0.00\n",
      "mean_reward1:-1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:-1.00 +/- 0.00\n",
      "mean_reward2:1.00 +/- 0.00\n",
      "mean_reward1:-1.00 +/- 0.00\n",
      "mean_reward2:1.00 +/- 0.00\n",
      "mean_reward1:-1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n",
      "mean_reward1:1.00 +/- 0.00\n",
      "mean_reward2:-1.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(60):\n",
    "    \n",
    "    if i%50==0:\n",
    "        models1.append(m11)\n",
    "        models2.append(m12)\n",
    "    m11  = DDPG.load(\"model1\",env = env1,verbose = 0)\n",
    "    m11.learn(total_timesteps=1000)\n",
    "    mean_reward1, std_reward1 = evaluate_policy(m11, env1, n_eval_episodes=100)\n",
    "    m11.save(\"model1\")\n",
    "    print(f\"mean_reward1:{mean_reward1:.2f} +/- {std_reward1:.2f}\")\n",
    "    \n",
    "    m12 = DDPG.load(\"model2\", env = env2, verbose=0)\n",
    "    m12.learn(total_timesteps=1000)\n",
    "    mean_reward2, std_reward2 = evaluate_policy(m12, env2, n_eval_episodes=100)\n",
    "    m12.save(\"model2\")\n",
    "    print(f\"mean_reward2:{mean_reward2:.2f} +/- {std_reward2:.2f}\")\n",
    "    \n",
    "    env1 = Kickenv1(0.03,0.01,0.7,0.01,m12)\n",
    "    env2 = Kickenv2(0.03,0.01,0.7,0.01,m11)\n",
    "    rewards1.append(mean_reward1)\n",
    "    rewards2.append(mean_reward2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93743bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.plot(rewards1)\n",
    "plt.plot(rewards2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ec8e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "544eba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#for rendering purposes\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.core import ObservationWrapper\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pygame\n",
    "import sys\n",
    "\n",
    "from pygame import Color, display, draw, event, font, key, mouse, Rect, Surface, time\n",
    "\n",
    "class Kickenv(gym.Env):\n",
    "    \n",
    "    def __init__(self,di :float,vp : float,lenbar :float, wbar:float, display_width = 750 + 50, display_height = 750 + 50, fps=1):\n",
    "\n",
    "        '''Constructor for our environment. Should take any relevant parameters as arguments.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        di : float\n",
    "            diameter of puck\n",
    "        vp : float\n",
    "            horizontal velocity of puck\n",
    "        lenbar : float \n",
    "            length of the bar\n",
    "        wbar : float \n",
    "            width of the bar\n",
    "\n",
    "        Ideal Settings :\n",
    "        BallDiameter = ScreenWidth / 64;\n",
    "        BarWidth = Settings.BallDiameter / 2;\n",
    "        BarLength = ScreenHeight / 6;\n",
    "        BallStartingPosX = ScreenWidth / 8;\n",
    "        BallStartingPosY = ScreenHeight / 2;\n",
    "        BarStartingPosX = 7*ScreenWidth/8;\n",
    "        BarStartingPosY = BallStartingPosY;\n",
    "        FinalLine =BarStartingPosX + 3*BarWidth;        \n",
    "        BallSpeed = ScreenWidth / 100\n",
    "        '''\n",
    "        # For simplicity only allow odd number of states.\n",
    "        self.xi=-0.75\n",
    "        self.yi=0\n",
    "        \n",
    "        self.xib=0.75\n",
    "        self.yib=0\n",
    "        self.r=di/2\n",
    "        self.g=0.77\n",
    "        self.vbar=0\n",
    "        self.theta=1\n",
    "        self.vp=vp\n",
    "        self.lenbar=lenbar\n",
    "        self.wbar=wbar    \n",
    "        self.statep=[self.xi,self.yi]\n",
    "        self.stateb=[self.xib,self.yib]\n",
    "        self.timestepcounter=0\n",
    "        self.theta_arr=np.zeros(math.ceil((self.g-self.xi)/self.vp))\n",
    "        self.action_spacep=spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.action_spaceb=spaces.Box(low=-1,high=1,shape=(1,),dtype=np.float32)\n",
    "        self.observation_spacep=spaces.Box(low=np.array([-1.0+self.r,-1.0+self.r]),high=np.array([1.0-self.r,1.0-self.r]),shape=(2,))\n",
    "        self.observation_spaceb=spaces.Box(low=-1+self.lenbar/2,high=1-(self.lenbar/2),shape=(1,))\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        self.rewardp=0\n",
    "        self.statepr=self.statep\n",
    "        self.rewardb=0\n",
    "        \n",
    "        pygame.init()\n",
    "        display.init()\n",
    "\n",
    "        self.display_width = display_width\n",
    "        self.display_height = display_height\n",
    "\n",
    "        self.currentDisplaySurface = display.set_mode((display_width, display_height))\n",
    "        \n",
    "        display.set_caption(\"Penalty Shot Game\")\n",
    "\n",
    "        self.Clock = time.Clock()  \n",
    "        self.fps = fps\n",
    "\n",
    "        self.black = (0,0,0)\n",
    "        self.white = (255,255,255)\n",
    "        self.cellgreen = (0,230,0)\n",
    "\n",
    "        self.currentDisplaySurface.fill((255, 255, 255))\n",
    "        self.puckRad = int(self.r * 375)\n",
    "        \n",
    "        self.makeTable()\n",
    "        self.goalLine()\n",
    "        self.makePuck()\n",
    "        self.makeBar()\n",
    "        self.startLine()\n",
    "\n",
    "        \n",
    "    def step(self,action) :\n",
    "        '''Defines what to do if an action is taken.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : list of actions taken by the puck and bar respectively\n",
    "            Action  taken.  \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[int, float, bool, None]\n",
    "            A tuple containing the next state, reward obtained, whether terminal state has been reached, and None.\n",
    "        '''\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        posbar=self.stateb\n",
    "        self.theta_arr[self.timestepcounter]=action[1]\n",
    "        action[0]=action[0]*self.vp\n",
    "        if(self.timestepcounter>=2):\n",
    "            if action[1]>=0.8:\n",
    "                if self.theta_arr[self.timestepcounter-1]>=0.8 and self.theta_arr[self.timestepcounter-2]>=0.8 :\n",
    "                    self.theta=self.theta+0.85\n",
    "            elif action[1]<=-0.8 :\n",
    "                if self.theta_arr[self.timestepcounter-1]<=-0.8 and self.theta_arr[self.timestepcounter-2]<=0.8 :\n",
    "                    self.theta=self.theta+0.85\n",
    "            else : self.theta=1\n",
    "        else : self.theta=1            \n",
    "        vw=self.theta*self.vp*(2/3)\n",
    "        statebr=self.stateb\n",
    "        self.stateb[1]=max(self.observation_spaceb.low,min(self.observation_spaceb.high,self.stateb[1]+action[1]*vw))\n",
    "        self.statepr=self.statep\n",
    "        self.statep[0]=self.statep[0]+self.vp\n",
    "        self.statep[1]=max(self.observation_spacep.low[1],min(self.observation_spacep.high[1],self.statep[1]+action[0]))\n",
    "        self.rewardp=0\n",
    "        self.rewardb=0\n",
    "        if abs(self.statep[0]-posbar[0])<=self.r+self.wbar/2 and abs(self.statep[1]-posbar[1])<=self.r+self.lenbar/2 :\n",
    "            self.rewardp=-1\n",
    "            self.rewardb=1\n",
    "            self.donep=True\n",
    "        if self.statep[0]>=self.xib and self.statepr[0]<=self.xib and self.donep==False:\n",
    "            self.donep=True\n",
    "            self.rewardp=1\n",
    "            self.rewardb=-1\n",
    "            self.statep[0]=self.xib-self.wbar-self.r \n",
    "        if self.statep[0]+self.r>=self.g and self.donep==False:\n",
    "            self.donep=True\n",
    "            self.rewardp=1\n",
    "            self.rewardb=-1 \n",
    "            \n",
    "\n",
    "        return self.statep,self.stateb,self.rewardp,self.rewardb,self.donep,self.doneb,None\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''What to do if we reset the environment.\n",
    "        '''\n",
    "        # We simply make our current state equal to initial state .\n",
    "        # We change the status of completion of our episode[self.done variable] to False.\n",
    "        # make timestepcounter=0\n",
    "        #change parameters such a theta and prev_dir to 1 and 0 respectively.\n",
    "        # Return state and status of completion \n",
    "        self.statep =[self.xi,self.yi]\n",
    "        self.donep=False\n",
    "        self.doneb=False\n",
    "        self.stateb=[self.xib,self.yib]\n",
    "        self.prev_dir=0\n",
    "        self.theta=1\n",
    "        self.theta_arr=np.zeros(math.ceil((self.g-self.xi)/self.vp))\n",
    "        self.statepr=self.statep\n",
    "        self.timestepcounter=0\n",
    "        return self.statep,self.stateb,self.donep,self.doneb    \n",
    "\n",
    "    def render(self, display_width = 750 + 50, display_height = 750 + 50, fps=1):\n",
    "\n",
    "        self.Clock.tick(self.fps)\n",
    "        display.update()\n",
    "\n",
    "        puckx, pucky = self.statep\n",
    "        bary = self.stateb[1]\n",
    "\n",
    "        self.puckX = (puckx+1)*375 + 25\n",
    "        self.puckY = (pucky-1)*(-375) + 25\n",
    "        self.barY = (bary-1)*(-375) + 25\n",
    "\n",
    "        self.currentDisplaySurface.fill((255, 255, 255))\n",
    "        self.makeTable()\n",
    "        self.goalLine()\n",
    "        self.startLine()\n",
    "\n",
    "        self.shiftPuck()\n",
    "        self.shiftBar()\n",
    "        \n",
    "    def makeTable(self):\n",
    "        borderWidth = 1\n",
    "        top = 25\n",
    "        left = 25\n",
    "\n",
    "        tableRect = Rect(left , top , self.display_width - 50, self.display_height - 50)\n",
    "\n",
    "        draw.rect(self.currentDisplaySurface, self.black, tableRect, borderWidth)\n",
    "\n",
    "    def goalLine(self):\n",
    "        lineWidth = 1\n",
    "        borderWidth = 1\n",
    "        if (borderWidth > self.vp * 375):\n",
    "            borderWidth = self.vp * 375\n",
    "\n",
    "        self.goalx = (self.g + 1)*375 + 25\n",
    "\n",
    "        draw.line(self.currentDisplaySurface, (100,100,150), (self.goalx, 25), (self.goalx, self.display_height - 25), lineWidth)\n",
    "\n",
    "    def startLine(self):\n",
    "        lineWidth = 1\n",
    "        borderWidth = 10\n",
    "\n",
    "        self.startx = (-0.75 + 1)*375 + 25\n",
    "        draw.line(self.currentDisplaySurface, (100,100,150), ( self.startx, 25), ( self.startx, self.display_height - 25), lineWidth)\n",
    "\n",
    "    def makePuck(self):\n",
    "        self.xi=-0.75\n",
    "        self.yi=0\n",
    "        self.puckXinitial = (-0.75 + 1)*375 + 25 - self.puckRad\n",
    "        self.puckYinitial = (0 + 1)*375 + 25\n",
    "\n",
    "        self.puckX = self.puckXinitial\n",
    "        self.puckY = self.puckYinitial\n",
    "        \n",
    "        draw.circle(self.currentDisplaySurface, (50,50,250), (self.puckXinitial ,self.puckYinitial), self.puckRad)\n",
    "\n",
    "    def makeBar(self):\n",
    "        self.barXinitial = (0.75 + 1)*375 + (self.wbar*375) + 25\n",
    "        self.barYinitial = (0+1)*375 - (self.lenbar/2)*375 + 25\n",
    "\n",
    "        self.barX = self.barXinitial\n",
    "        self.barY = self.barYinitial\n",
    "        \n",
    "        tableRect = Rect(self.barXinitial , self.barYinitial ,(self.wbar)*375, (self.lenbar)*375)\n",
    "        draw.rect(self.currentDisplaySurface, (250,50,50), tableRect)\n",
    "\n",
    "    def shiftPuck(self):\n",
    "        draw.circle(self.currentDisplaySurface, (50,50,250), (self.puckX ,self.puckY), self.puckRad)\n",
    "\n",
    "    def shiftBar(self):\n",
    "        self.barXinitial = (0.75 + 1)*375 + (self.wbar*375) + 25\n",
    "        tableRect = Rect(self.barXinitial, self.barY,(self.wbar)*375, (self.lenbar)*375)\n",
    "        draw.rect(self.currentDisplaySurface, (250,50,50), tableRect)\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit() #Uninitilize all imported pygame modules.\n",
    "\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9fa69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.8/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env = Kickenv(0.03,0.3,0.7,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7154684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
